# ğŸš€ PrÃ¡ctica Kubernetes - GestiÃ³n de Archivos Distribuida

[![Kubernetes](https://img.shields.io/badge/Kubernetes-v1.28-blue?logo=kubernetes)](https://kubernetes.io/)
[![Docker](https://img.shields.io/badge/Docker-20.10-blue?logo=docker)](https://www.docker.com/)
[![AWS EC2](https://img.shields.io/badge/AWS-EC2-orange?logo=amazon-aws)](https://aws.amazon.com/ec2/)

---

## ğŸ“‹ Tabla de Contenidos

- [Objetivo de la PrÃ¡ctica](#objetivo-de-la-prÃ¡ctica)
- [Arquitectura del Sistema](#arquitectura-del-sistema)
- [PreparaciÃ³n del Entorno](#1-preparaciÃ³n-del-entorno)
- [ImÃ¡genes Docker](#2-imÃ¡genes-docker)
- [Deployments](#deployments)
- [Servicios](#servicios)
- [EjecuciÃ³n](#ejecuciÃ³n)
- [ConfiguraciÃ³n Avanzada 1: HostPath](#configuraciÃ³n-avanzada-1)
- [ConfiguraciÃ³n Avanzada 2: NFS](#ğŸš€-parte-3-configuraciÃ³n-avanzada-con-nfs)
- [Pruebas y VerificaciÃ³n](#âœ…-parte-4-pruebas-y-verificaciÃ³n)

---

## ğŸ¯ Objetivo de la PrÃ¡ctica

Esta prÃ¡ctica tiene como objetivo desplegar una **aplicaciÃ³n distribuida de gestiÃ³n de archivos** utilizando [Kubernetes](https://kubernetes.io/) y [Docker](https://www.docker.com/) en un clÃºster de instancias [AWS EC2](https://aws.amazon.com/ec2/).

### ğŸ—ï¸ Arquitectura del Sistema

Un clÃºster de Kubernetes estÃ¡ compuesto por:

#### **Control Plane (Nodo Maestro)**
- ğŸ›ï¸ **FunciÃ³n**: Coordina y supervisa todo el clÃºster
- ğŸ“Š **Responsabilidades**:
  - PlanificaciÃ³n de pods
  - GestiÃ³n del estado del clÃºster
  - CoordinaciÃ³n de nodos worker
  - API Server para comunicaciÃ³n

#### **Worker Nodes (Nodos Trabajadores)**
- âš™ï¸ **FunciÃ³n**: Ejecutan las aplicaciones containerizadas
- ğŸ“¦ **Componentes desplegados**:
  - **brokerFileManager**: Gestiona la comunicaciÃ³n entre clientes y servidores
  - **serverFileManager**: Proporciona acceso y almacenamiento de archivos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Control Plane                        â”‚
â”‚              (k8smaster0.psdi.org)                      â”‚
â”‚                                                         â”‚
â”‚  â€¢ API Server  â€¢ Scheduler  â€¢ Controller Manager       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker Node 1 â”‚      â”‚  Worker Node 2  â”‚
â”‚   (Broker)     â”‚      â”‚   (Server)      â”‚
â”‚                â”‚      â”‚                 â”‚
â”‚  Pod: Broker   â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Pod: Server    â”‚
â”‚  Port: 32002   â”‚      â”‚  Port: 32001    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. ğŸ› ï¸ PreparaciÃ³n del Entorno

### ğŸŒ Despliegue en AWS EC2

La infraestructura consta de **3 instancias EC2**:

| Instancia | Rol | FunciÃ³n | Hostname |
|-----------|-----|---------|----------|
| ğŸ›ï¸ **Control-Plane** | Master | Gestiona el clÃºster de Kubernetes | `k8smaster0.psdi.org` |
| ğŸ”— **Broker** | Worker | Ejecuta el servicio `brokerFileManager` | `k8sslave1.psdi.org` |
| ğŸ“ **Server** | Worker | Ejecuta el servicio `serverFileManager` | `k8sslave2.psdi.org` |

### ğŸ³ Rol de Kubernetes

[Kubernetes](https://kubernetes.io/docs/concepts/) gestiona los contenedores Docker que ejecutan:
- `brokerFileManager`: Orquesta las conexiones
- `serverFileManager`: Almacena y sirve archivos

> ğŸ’¡ **Nota**: AsegÃºrate de que todas las instancias tengan los [Security Groups](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) correctamente configurados para permitir comunicaciÃ³n entre puertos 32001 y 32002.

## 2. ğŸ³ ImÃ¡genes Docker

### ğŸ“ Â¿QuÃ© es un Dockerfile?

Un [Dockerfile](https://docs.docker.com/engine/reference/builder/) es un archivo de texto que contiene todas las instrucciones necesarias para construir una imagen Docker. Define:
- El sistema operativo base
- Las dependencias a instalar
- Los archivos a copiar
- Los comandos a ejecutar al iniciar el contenedor

### ğŸ“„ Dockerfile del Server (`serverFileManager`)

**Archivo**: `DockerfileS`

```dockerfile
# Imagen base oficial de Ubuntu 20.04
FROM ubuntu:20.04

# Actualizar repositorios e instalar dependencias necesarias
RUN apt-get update && \
    apt-get install -y software-properties-common curl && \
    rm -rf /var/lib/apt/lists/*

# Exponer el puerto 32001 para conexiones externas
EXPOSE 32001

# Copiar el ejecutable del servidor
COPY serverFileManager /

# Dar permisos de ejecuciÃ³n
RUN chmod +x /serverFileManager

# Crear directorio para almacenar archivos
RUN mkdir FileManagerDir

# Copiar configuraciÃ³n DNS
COPY resolv.conf /

# Comando de inicio del servidor
# Argumentos: <IP_BROKER> <PUERTO_BROKER> <IP_PUBLICA> <PUERTO_SERVER>
CMD cp resolv.conf /etc/resolv.conf && \
    /serverFileManager 172.31.31.163 32002 $(curl -s https://api.ipify.org) 32001
```

#### ğŸ”‘ ParÃ¡metros del Servidor

| ParÃ¡metro | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| `IP_BROKER` | `172.31.31.163` | âš ï¸ **IP privada del Broker** (modificar segÃºn tu despliegue) |
| `PUERTO_BROKER` | `32002` | Puerto donde escucha el Broker |
| `IP_PUBLICA` | `$(curl -s https://api.ipify.org)` | Obtiene la IP pÃºblica automÃ¡ticamente |
| `PUERTO_SERVER` | `32001` | Puerto donde escucha el Servidor |

> âš ï¸ **Importante**: Reemplaza `172.31.31.163` con la IP privada real de tu instancia Broker.

### ğŸ“„ Dockerfile del Broker (`brokerFileManager`)

**Archivo**: `DockerfileB`

```dockerfile
# Imagen base oficial de Ubuntu 20.04
FROM ubuntu:20.04

# Actualizar repositorios e instalar dependencias
RUN apt-get update && \
    apt-get install -y software-properties-common curl && \
    rm -rf /var/lib/apt/lists/*

# Exponer el puerto 32002 para el servicio Broker
EXPOSE 32002

# Copiar el ejecutable del broker al contenedor
COPY brokerFileManager /brokerFileManager

# Dar permisos de ejecuciÃ³n
RUN chmod +x /brokerFileManager

# Ejecutar el broker al iniciar el contenedor
CMD ["/brokerFileManager"]
```

#### ğŸ”— ComunicaciÃ³n Broker-Server

El `serverFileManager` necesita conectarse al `brokerFileManager` para:
- ğŸ“ Registrarse como servidor disponible
- ğŸ”„ Recibir informaciÃ³n de otros servidores
- ğŸ“¡ Mantener la comunicaciÃ³n activa

Esta comunicaciÃ³n se realiza a travÃ©s del **puerto 32002**, donde el Broker escucha las conexiones entrantes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Server  â”‚â”€â”€â”€â”€ 32002 â”€â”€â”€â”€â–ºâ”‚  Broker  â”‚
â”‚  :32001  â”‚â—„â”€â”€â”€ register â”€â”€â”‚  :32002  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Deployments de Kubernetes

### Â¿QuÃ© es un Deployment?

Un [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) en Kubernetes es un recurso que:
- âœ… Gestiona la creaciÃ³n y actualizaciÃ³n de pods automÃ¡ticamente
- ğŸ”„ Mantiene el nÃºmero deseado de rÃ©plicas ejecutÃ¡ndose
- ğŸ›¡ï¸ Proporciona alta disponibilidad y self-healing
- ğŸ“ˆ Permite escalado horizontal fÃ¡cil

### ğŸ”— Deployment del Broker

**Archivo**: `brokerDeployment.yml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: brokerfilemanager-deployment
  namespace: default
spec:
  replicas: 1  # NÃºmero de pods del Broker
  selector:
    matchLabels:
      app: brokerfilemanager  # Identifica los pods a gestionar
  template:
    metadata:
      labels:
        app: brokerfilemanager
    spec:
      nodeSelector:
        rol: broker  # Despliega SOLO en nodos con etiqueta rol=broker
      containers:
      - name: brokerfilemanager-deployment
        image: docker.io/bitboss629/brokerfilemanager:v1
        ports:
        - containerPort: 32002
```

#### ğŸ“ ExplicaciÃ³n de Campos

| Campo | Valor | DescripciÃ³n |
|-------|-------|-------------|
| `apiVersion` | `apps/v1` | VersiÃ³n de la API de Kubernetes |
| `kind` | `Deployment` | Tipo de recurso a crear |
| `replicas` | `1` | Cantidad de instancias (pods) a ejecutar |
| `nodeSelector` | `rol: broker` | Etiqueta que determina en quÃ© nodo se despliega |
| `image` | `bitboss629/brokerfilemanager:v1` | Imagen Docker desde [Docker Hub](https://hub.docker.com/) |

---

### serverDeployment:
````
apiVersion: apps/v1
kind: Deployment
metadata:
 name: serverfilemanager-deployment
 namespace: default
spec:
 replicas: 1
 selector:
  matchLabels:
   app: serverfilemanager
 template:
  metadata:
   labels:
    app: serverfilemanager
  spec:
   nodeSelector:
    rol: server
   containers:
   - name: serverfilemanager-deployment
     image: docker.io/bitboss629/serverfilemanager:v1 
````
---
````
image: docker.io/bitboss629/brokerfilemanager:v1
````

ESO significa:

  - â€œKubernetes buscarÃ¡ la imagen bitboss629/brokerfilemanager:v1 en Docker Hubâ€

  - Si no existe â†’ FALLA

  - Si existe â†’ Kubernetes la descarga y levanta los pods

Antes de esto debemos de asignar a nuestros pods unos roles para relacionarlos con los deployments:
````
  kubectl label node k8smaster0.psdi.org rol=broker
  kubectl label node k8sslave1.psdi.org rol=server
````
![nodo](https://github.com/rodrigomhz/PracticaKubernetes/blob/main/Images/nodes.png)

## Servicios

En Kubernetes, los Services se utilizan para exponer los puertos de los pods de manera consistente y permitir la comunicaciÃ³n entre ellos. Existen diferentes tipos de servicios, pero en este caso estamos usando NodePort, lo que significa que los servicios estarÃ¡n accesibles desde fuera del clÃºster a travÃ©s de un puerto especÃ­fico.

### brokerService.
````
apiVersion: v1
kind: Service
metadata:
 name: brokerservice
 namespace: default
spec:
 type: NodePort
 selector:
  app: brokerfilemanager
 ports:
 # Enlazamos el puerto 32002 interno al externo 32002
 #Virtual
  - port: 32002 # Puerto al que se accederÃ¡ dentro del contenedor
    targetPort: 32002 # El puerto dentro del pod al que el servicio debe redirigir
    # Fisico
    nodePort: 32002 # Puerto expuesto externamente en el nodo
 externalTrafficPolicy: Cluster
````

### serverService
````
apiVersion: v1
kind: Service
metadata:
 name: serverservice
 namespace: default
spec:
 type: NodePort
 selector:
  app: serverfilemanager
 ports:
  - port: 32001 # Puerto dentro del clÃºster para el servidor
    targetPort: 32001 # El puerto dentro del pod al que el servicio debe redirigir
    nodePort: 32001 # Puerto expuesto externamente en el nodo para el servicio
 externalTrafficPolicy: Cluster
````
Resumen Conceptos:

  1. name: Es el nombre del servicio. Como si le pones un nombre a la puerta del servicio, para poder llamarla.
  
  2. app: Es una etiqueta para identificar a quÃ© grupo de casitas (pods) pertenece este servicio. Es como si dijeras: "Estas casitas son del tipo broker".
  
  3. port: Es el puerto dentro del clÃºster que otros servicios usan para acceder a tu servicio. Es la puerta que los servicios dentro de Kubernetes usarÃ¡n para conectarse.
  
  4. targetPort: Es el puerto dentro del contenedor donde el servicio realmente estÃ¡ esperando las conexiones. Es como la puerta dentro de la casita.
  
  5. nodePort: Es el puerto en los nodos del clÃºster que se expone a fuera del clÃºster. Es como si pusieras una puerta de entrada en la muralla del barrio para que la gente desde fuera pueda entrar.

# EjecucuiÃ³n
---
Antes de desplegar nuestros servicios en Kubernetes, es necesario construir las imÃ¡genes Docker de ambos componentes: brokerFileManager y serverFileManager.
Estas imÃ¡genes son las que luego utilizarÃ¡n los Deployments.

Antes de comenzar debemos loguearnos:
````
docker login
````
Te pedirÃ¡:

  - Username â†’ tu usuario de Docker Hub

  - Password â†’ tu contraseÃ±a de Docker Hub

## 1. ConstrucciÃ³n y subida de la imagen del Broker

Primero, desde la carpeta que contiene el Dockerfile del broker, construimos su imagen:

````
docker build -t bitboss629/brokerfilemanager:v1 .
````

Una vez generada, la subimos a Docker Hub para que Kubernetes pueda descargarla:
````
docker push bitboss629/brokerfilemanager:v1
````

## 2. ConstrucciÃ³n y subida de la imagen del Server

A continuaciÃ³n, repetimos el proceso para el servidor, situÃ¡ndonos en la carpeta donde estÃ¡ su Dockerfile:
````
docker build -t bitboss629/serverfilemanager:v1 .
````

Subimos tambiÃ©n esta imagen a Docker Hub:
````
docker push bitboss629/serverfilemanager:v1
````

Podemos comprobar que todo ha ido bien con el comando:
````
docker images
````

![nodo](https://github.com/rodrigomhz/PracticaKubernetes/blob/main/Images/dockerImages.png)

## 3. AplicaciÃ³n de los Deployments en Kubernetes

Con ambas imÃ¡genes ya disponibles en Docker Hub, podemos desplegar los servicios en el clÃºster usando los archivos YAML correspondientes:
````
kubectl apply -f brokerDeployment.yml
kubectl apply -f serverDeployment.yml
````

Esto crearÃ¡ los pods y asignarÃ¡ las imÃ¡genes construidas previamente a los Deployments, de forma que cada servicio pueda ejecutarse dentro del clÃºster Kubernetes

## 4. Aplicacion de los Service en Kubernetes

Ahora que los pods existen, ya puedes exponerlos mediante los servicios:
````
kubectl apply -f brokerService.yml
kubectl apply -f serverService.yml
````
---

## Â¿Por quÃ© en este orden?

### ğŸ“Œ Deployments â†’ crean los pods

  - Sin pods, los services no tienen a quiÃ©n conectar.

### ğŸ“Œ Services â†’ exponen los pods

  - Una vez los pods existen, puedes "publicarlos" dentro y fuera del clÃºster.


## CONFIGURACIÃ“N AVANZADA 1

### ğŸŸ¦ Objetivo

Tener varios pods del serverFileManager ejecutÃ¡ndose en el mismo nodo y compartiendo una misma carpeta para que todos los clientes vean los mismos archivos sin importar quÃ© pod atienda la peticiÃ³n.

Â¿Por quÃ© funciona hostPath aquÃ­?

Porque hostPath monta una carpeta del nodo fÃ­sico dentro de cada pod.
Como los pods estÃ¡n en el mismo nodo, todos montan la misma carpeta:

````
EC2 nodo esclavo
â”‚
â”œâ”€â”€ /mnt/data   â† carpeta fÃ­sica del nodo
â”‚     â”œâ”€ archivo1.txt
â”‚     â”œâ”€ archivo2.png
â”‚
â”œâ”€â”€ pod1 â†’ monta /mnt/data en /data
â”œâ”€â”€ pod2 â†’ monta /mnt/data en /data
â””â”€â”€ pod3 â†’ monta /mnt/data en /data
````

  âœ” TODOS ven lo mismo

  âœ” TODOS escriben/leen lo mismo

  âœ” NUNCA se borra si un pod cae

  âœ” Cumple exactamente la ConfiguraciÃ³n Avanzada 1

Esa carpeta /mnt/data hay que crearla en el nodo esclavo, NO en el control-plane

---

### 1. Crear la carpeta compartida en el nodo esclavo

Nos conectamos a la mÃ¡quina de server:
````
ssh -i labsuser.pem ubuntu@k8sslave2.psdi.org
````

En el nodo donde se ejecutarÃ¡n los pods del server:
````
sudo mkdir -p /mnt/server-data
sudo chmod 777 /mnt/server-data
````

Comprobamos los nodos:
````
kubectl get nodes
````

Verificamos la etiqueta:
````
kubectl get nodes --show-labels
````

### 2. Deployment del serverFileManager con hostPath + nodeSelector
````
apiVersion: apps/v1
kind: Deployment
metadata:
  name: serverfilemanager-deployment
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: serverfilemanager
  template:
    metadata:
      labels:
        app: serverfilemanager
    spec:
      nodeSelector:
        rol: server
      containers:
      - name: serverfilemanager
        image: docker.io/bitboss629/serverfilemanager:v1
        volumeMounts:
        - name: server-storage
          mountPath: /FileManagerDir
      volumes:
      - name: server-storage
        hostPath:
          path: /home/ubuntu/compartido
          type: DirectoryOrCreate
````
Resumen Conceptos:

  - hostPath.path â†’ carpeta REAL del nodo

  - mountPath â†’ carpeta dentro del contenedor

  - replicas: 3 â†’ 3 pods usando la MISMA carpeta

  - nodeSelector â†’ obliga a que los pods estÃ©n en el nodo marcado

---

### 3. Hacer imagen de docker
````
docker build -t bitboss629/serverfilemanager:v1 .
docker push bitboss629/serverfilemanager:v1
````

1. Aplicar el deployment
````
kubectl apply -f serverDeployment.yml
````

2. Comprobar que funciona
````
kubectl get pods -o wide
````
Los 3 pods deben estar en el MISMO nodo.

Entrar en un pod:
````
kubectl exec -it <nombre-pod> -- bash
ls /data
````

Entrar en  otro pod:
````
kubectl exec -it <otro-pod> -- bash
ls /data
````

âœ” Deben aparecer los mismos archivos
âœ” Lo que suba un cliente al pod1 aparece en pod2 y pod3
âœ” Todo se guarda en /mnt/server-data del nodo esclavo

## CONCLUSIÃ“N
Esto completa la ConfiguraciÃ³n Avanzada 1 tal y como exige la prÃ¡ctica

ğŸš€ Parte 3: ConfiguraciÃ³n Avanzada con NFS
Esta secciÃ³n implementa almacenamiento compartido NFS para alcanzar la mÃ¡xima calificaciÃ³n (10). Permite tener 3 rÃ©plicas del servidor compartiendo los mismos archivos.

Â¿Por quÃ© NFS?
âœ… Alta disponibilidad: 3 rÃ©plicas del servidor corriendo simultÃ¡neamente
âœ… Persistencia: Los archivos se mantienen aunque un pod se reinicie
âœ… ComparticiÃ³n: Todas las rÃ©plicas ven los mismos archivos en tiempo real
âœ… Escalabilidad: FÃ¡cil aÃ±adir mÃ¡s rÃ©plicas sin perder datos
3.1 Instalar y Configurar Servidor NFS
En k8smaster0:

# Actualizar repositorios
sudo apt-get update

# Instalar servidor NFS
sudo apt-get install -y nfs-kernel-server

# Crear directorio compartido
sudo mkdir -p /mnt/nfs-filemanager

# Configurar permisos
sudo chown nobody:nogroup /mnt/nfs-filemanager
sudo chmod 777 /mnt/nfs-filemanager
Configurar exportaciÃ³n NFS:

# AÃ±adir al archivo de exportaciones
echo "/mnt/nfs-filemanager *(rw,sync,no_subtree_check,no_root_squash)" | sudo tee -a /etc/exports

# Aplicar configuraciÃ³n
sudo exportfs -ra

# Verificar que el export estÃ¡ activo
sudo exportfs -v
Salida esperada:

/mnt/nfs-filemanager
        <world>(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)
Verificar servicio:

sudo systemctl status nfs-kernel-server
âš ï¸ Problema ComÃºn 1: Ruta sin barra inicial

Si ves el error exportfs: Failed to stat mnt/nfs-filemanager: No such file or directory:

# Eliminar lÃ­nea incorrecta
sudo sed -i '/^mnt\/nfs-filemanager/d' /etc/exports

# AÃ±adir correctamente (con / inicial)
echo "/mnt/nfs-filemanager *(rw,sync,no_subtree_check,no_root_squash)" | sudo tee -a /etc/exports

# Aplicar
sudo exportfs -ra
3.2 Instalar Cliente NFS en Workers
En k8sslave2 (usando kubectl debug desde k8smaster0):

# Entrar al nodo con chroot
kubectl debug node/k8sslave2.psdi.org -it --image=ubuntu -- chroot /host bash

# Dentro del nodo, instalar cliente NFS
apt-get update
apt-get install -y nfs-common

# Salir
exit
Verificar instalaciÃ³n:

kubectl debug node/k8sslave2.psdi.org -it --image=ubuntu -- chroot /host bash -c "dpkg -l | grep nfs-common"
Salida esperada:

ii  nfs-common  1:2.6.1-1ubuntu1.2  amd64  NFS support files common to client and server
ğŸ’¡ Nota: Los pods de debug temporales pueden eliminarse despuÃ©s:

kubectl delete pod -l app=node-debugger
3.3 Configurar Security Groups de AWS para NFS
Antes de crear los recursos de Kubernetes, debes abrir los puertos NFS en AWS:

Ve a AWS Console â†’ EC2 â†’ Security Groups
Selecciona el security group de k8smaster0
Editar reglas de entrada y aÃ±adir:
Tipo	Protocolo	Puerto	Origen	DescripciÃ³n
TCP personalizado	TCP	2049	172.31.0.0/16	NFS
TCP personalizado	TCP	111	172.31.0.0/16	RPC (portmapper)
âš ï¸ Problema ComÃºn 2: Connection timed out al montar NFS

Si los pods muestran mount.nfs: Connection timed out, es porque el Security Group bloquea los puertos NFS. AsegÃºrate de aÃ±adir las reglas anteriores.

3.4 Crear PersistentVolume NFS
Archivo: pv-nfs.yml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: server-pv-nfs
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany  # Permite que mÃºltiples pods lo usen simultÃ¡neamente
  nfs:
    server: 172.31.64.84  # IP privada de k8smaster0
    path: /mnt/nfs-filemanager
  storageClassName: nfs
Crear el archivo y aplicarlo:

cd ~/Practica2/SERVIDOR_NFS

# Crear el archivo pv-nfs.yml con el contenido anterior
nano pv-nfs.yml

# Aplicar
kubectl apply -f pv-nfs.yml

# Verificar
kubectl get pv
Salida esperada:

NAME            CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM
server-pv-nfs   5Gi        RWX            Retain           Available
3.5 Crear PersistentVolumeClaim NFS
Archivo: pvc-nfs.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: server-pvc-nfs
spec:
  accessModes:
    - ReadWriteMany  # Debe coincidir con el PV
  resources:
    requests:
      storage: 5Gi  # Solicita 5Gi
  storageClassName: nfs  # Debe coincidir con el PV
Aplicar:

# Crear el archivo
nano pvc-nfs.yml

# Aplicar
kubectl apply -f pvc-nfs.yml

# Verificar que se vinculÃ³ al PV
kubectl get pvc
kubectl get pv
Salida esperada:

NAME             STATUS   VOLUME          CAPACITY   ACCESS MODES
server-pvc-nfs   Bound    server-pv-nfs   5Gi        RWX
Estado Bound significa que el PVC encontrÃ³ el PV y estÃ¡ listo para usar.

3.6 Actualizar Deployment del Servidor con NFS
Ahora modifica el DeploymentServer.yml para usar el volumen NFS y escalar a 3 rÃ©plicas:

apiVersion: apps/v1
kind: Deployment
metadata:
 name: server-deployment
 namespace: default
spec:
 replicas: 3  # â† CAMBIO: Escalar de 1 a 3 rÃ©plicas
 selector:
  matchLabels:
   app: server-deploy
 template:
  metadata:
   labels:
    app: server-deploy
  spec:
   nodeSelector:
    node-role: server
   containers:
   - name: server-deployment
     image: docker.io/d1n0s/kubernetes-practica2server:v2
     ports:
     - containerPort: 32001
     volumeMounts:  # â† NUEVO: Montar el volumen NFS
     - name: filemanager-storage-nfs
       mountPath: /FileManagerDir  # Donde la app guarda archivos
   volumes:  # â† NUEVO: Definir el volumen desde el PVC
   - name: filemanager-storage-nfs
     persistentVolumeClaim:
       claimName: server-pvc-nfs  # Referencia al PVC creado
Aplicar los cambios:

# Eliminar el deployment anterior
kubectl delete deployment server-deployment

# Aplicar el nuevo con NFS
kubectl apply -f DeploymentServer.yml

# Observar cÃ³mo se crean las 3 rÃ©plicas
kubectl get pods -w
âš ï¸ Problema ComÃºn 3: ImagePullBackOff

Si los pods muestran ImagePullBackOff, verifica la versiÃ³n de la imagen:

# Ver el error
kubectl describe pod server-deployment-xxx

# Si dice que no encuentra v3, verifica el deployment
kubectl get deployment server-deployment -o yaml | grep image

# Debe ser v2 (que existe en Docker Hub)
# Si estÃ¡ mal, edita DeploymentServer.yml y vuelve a aplicar
âš ï¸ Problema ComÃºn 4: MountVolume.SetUp failed - Connection timed out

Este es el problema mÃ¡s comÃºn al configurar NFS. Los pods quedan en ContainerCreating:

# Ver el error
kubectl describe pod server-deployment-xxx
Causa: Security Group de AWS bloqueando puertos NFS.

SoluciÃ³n: AÃ±adir reglas en Security Group (ver secciÃ³n 3.3).

Salida esperada cuando todo funciona:

NAME                                 READY   STATUS    NODE
broker-deployment-6fd556654c-jzdsx   1/1     Running   k8sslave1.psdi.org
server-deployment-6bc5f558c5-7vpf9   1/1     Running   k8sslave2.psdi.org
server-deployment-6bc5f558c5-cvltl   1/1     Running   k8sslave2.psdi.org
server-deployment-6bc5f558c5-q9j**   1/1     Running   k8sslave2.psdi.org
âœ… Parte 4: Pruebas y VerificaciÃ³n
Esta secciÃ³n documenta las pruebas realizadas para verificar que el sistema funciona correctamente con NFS.

4.1 Verificar Estado del Cluster
# Ver todos los pods
kubectl get pods -o wide

# Ver servicios
kubectl get svc

# Ver volÃºmenes
kubectl get pv,pvc
4.2 Prueba de Persistencia de Archivos
Paso 1: Crear un archivo de prueba

cd ~/Practica2
echo "Esto es una prueba" > Prueba.txt
cat Prueba.txt  # Verificar contenido
Paso 2: Subir archivo al sistema

# Conectar al broker
./clientFileManager 172.31.31.30 32002

# Dentro del cliente, subir el archivo
upload Prueba.txt

# Verificar que se subiÃ³
lls
Salida esperada:

Enter command:
upload Prueba.txt
Coping file Prueba.txt in to the FileManager path
Reading file: Prueba.txt 19 bytes

Enter command:
lls
Listing files fileManager path
FileManagerDir/Prueba.txt
Paso 3: Salir y reconectar (puede conectar a otra rÃ©plica)

# Salir (Ctrl+C si "exit" no funciona)
# Volver a conectar
./clientFileManager 172.31.31.30 32002

# Listar archivos
lls
Resultado esperado: El archivo Prueba.txt debe seguir ahÃ­, confirmando la persistencia.

4.3 Verificar Archivos en el Servidor NFS
En k8smaster0:

# Ver archivos en el directorio NFS
ls -la /mnt/nfs-filemanager/

# Ver contenido del archivo
cat /mnt/nfs-filemanager/Prueba.txt
Salida esperada:

total 12
drwxrwxrwx 2 nobody nogroup 4096 Nov 26 17:45 .
drwxr-xr-x 3 root   root    4096 Nov 26 17:10 ..
-rw-r--r-- 1 nobody nogroup   19 Nov 26 17:45 Prueba.txt
4.4 Verificar Logs de las RÃ©plicas
# Obtener nombres de los pods
kubectl get pods | grep server-deployment

# Ver logs de cada rÃ©plica (reemplaza con tus nombres reales)
kubectl logs server-deployment-6bc5f558c5-7vpf9
kubectl logs server-deployment-6bc5f558c5-cvltl
kubectl logs server-deployment-6bc5f558c5-q9j**
DeberÃ­as ver que todas las rÃ©plicas estÃ¡n registradas en el broker y listas para recibir conexiones.
